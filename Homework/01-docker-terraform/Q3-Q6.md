# Data 
## taxi trips
`wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2025-11.parquet`
## taxi zones
`wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv`


# Plan

## Initial thoughts
To answer questions 3-6, we need to perform analysis on the data. Thus, to practice the skills with docker, we'll need an ingestion script deployed with a postgres database. We could skip the database but it helps with building out the docker network. 

1. Create Postgres DB using Docker Run
2. Create Jupyter Notebook script to pull the data interactively with another container. This container will need UV to run pandas, postgres library/driver. We'll bundle all these assets under a folder called pipeline. This would simulate sending it to someone alongside the relevant docker pieces. The script will only require populating the database. 
3. We'll use pgadmin to query the database, it seems easier than pgcli.

## Final Thoughts
So, overall, a network will be required with 3 docker containers. One for front and backend Postgres. We'll need another for the pipeline.py to execute to popluate the database. After this populates, we'll use pgAdmin to get the Answers.

# Solution Steps
1. docker-compose `Homework/01-docker-terraform/q3-q6_pipeline/docker-compose.yaml` to spin up network with pgadmin and postgres
2. uv init and uv add for pandas, pyscopg2-binary, and sqlalchemy. Library for data tables and sql insertion from script. uv add jupyter for the notebook and the subsequent conversion.
3. needed to use uv add pyarrow as we're ingesting parquet. Pd.read_parquet requires an engine i.e. pyarrow. Issues with pyarrow==23.00. Needed to install "pyarrow==22.0.0". 
4. jupyter notebook created, and used query tool in pgadmin to build sql queries to answer questions: `Homework/01-docker-terraform/q3-q6_pipeline/pgadmin_q3-q6_solutions.sql`
5. Now, to finish, need to convert notebook to python script using jupyter convert. Then, need to run this with a container executing it on start up and installing relevant libraries with UV.
6. Final docker-compose file `Homework/01-docker-terraform/q3-q6_pipeline/docker-compose.yaml`. ChatGPT helped with integrating the 3rd service. It struggled with UV but I pointed it to the astral docs which show UV in a docker file. From there, it built the service. We built it so it waited for pg_database to be ready and it checked it via the container itself rather than using depends_on which only waits for a container being created.